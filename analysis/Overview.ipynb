{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023283c6-974a-48c1-ad0e-927f2b7c53a8",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4078541-49b3-4e18-b361-74336e9a5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168aae0e-dbb0-4602-8163-cd5de47ada7c",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Loading pre-trained DeBERTa-v3 model for sequence classification and configure it using LoRA (Low-Rank Adaptation) with the PEFT (Parameter-Efficient Fine-Tuning) framework. Reducing the number of trainable parameters by freezing most of the model, making it computationally cheaper for testing and experimentation purposes.\n",
    "\n",
    "1. **Loading the Pre-trained DeBERTa-v3 Model**:\n",
    "   - The `AutoModelForSequenceClassification` class from Hugging Face’s `transformers` library is used to load the `microsoft/deberta-v3-base` model.\n",
    "   - The `num_labels=2` argument configures the model for a binary classification task. In this case, we assume the task is QQP (Quora Question Pairs), which is a common binary classification task.\n",
    "\n",
    "2. **Freezing the Base Model's Parameters**:\n",
    "   - All parameters of the base model (`model.base_model.parameters()`) are frozen. This means that only a small portion of the model will be trained, which is essential for reducing the number of parameters and compute cost. Freezing the majority of the model is a common practice in PEFT to focus only on adapting key layers.\n",
    "\n",
    "3. **LoRA Configuration**:\n",
    "   - **Why Use LoRA?** LoRA is a method to reduce the number of parameters that need to be trained by injecting low-rank matrices into key attention layers of the model.\n",
    "   - We define the `LoraConfig` as follows:\n",
    "     - `r=4`: This reduces the rank of the LoRA adaptation matrix to 4, meaning fewer additional parameters are added by LoRA. This is suitable for testing purposes to keep the training lightweight.\n",
    "     - `lora_alpha=16`: The scaling factor (LoRA alpha) is set to 16 to balance learning capacity while minimizing overfitting and controlling the overall parameter count.\n",
    "     - `target_modules=['query_proj', 'key_proj', 'value_proj']`: Here, we explicitly set the LoRA target modules to `query_proj`, `key_proj`, and `value_proj`. These correspond to the projections in the attention mechanism of the transformer model.\n",
    "   \n",
    "4. **LoRA Defaults:**\n",
    "   - **When `target_modules` Is Not Specified:**\n",
    "     - By default, if we do **not** provide the `target_modules` argument in LoRA, it will only apply LoRA to the `query_proj` and `value_proj` layers, **excluding** `key_proj`.\n",
    "     - **Why Only `query_proj` and `value_proj` by Default?** The `query_proj` and `value_proj` layers are the main drivers of attention computation, with the `query_proj` determining how tokens query attention across the sequence, and the `value_proj` determining what information is retrieved. These two layers have a larger impact on the model's ability to adapt to new tasks, which is why they are prioritized by default. \n",
    "     - **Exclusion of `key_proj`:** The `key_proj` layer, while important for calculating attention scores, does not introduce as much variability in model adaptation. By default, excluding `key_proj` reduces the number of trainable parameters, which is why it is often left out unless specified otherwise.\n",
    "\n",
    "5. **Including `key_proj` in `target_modules`:**\n",
    "   - In this configuration, we explicitly include `key_proj` to test its effect on model performance. Including `key_proj` adds more trainable parameters, which can sometimes lead to improved performance for complex tasks, though at the cost of additional memory and compute.\n",
    "\n",
    "6. **LoRA’s Focus on Attention Layers**:\n",
    "   - LoRA is specifically designed to modify attention layers, which are key to the transformer model’s ability to understand contextual relationships between tokens in a sequence.\n",
    "   - **Why Attention Layers?** These layers (particularly `query_proj` and `value_proj`) are central to how transformers handle attention—by modifying these layers, LoRA enables efficient task adaptation without retraining the entire model. Since attention layers govern how the model focuses on different parts of the input sequence, modifying them can lead to substantial improvements with minimal computational overhead.\n",
    "\n",
    "7. **Applying LoRA to the Model**:\n",
    "   - The `get_peft_model` function wraps the base model and applies LoRA to the specified attention modules. In this case, the LoRA layers are injected into `query_proj`, `key_proj`, and `value_proj`, while the rest of the model remains frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3284f50c-726f-489d-9f13-36dec916d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_model():\n",
    "    \"\"\"\n",
    "    Create a DeBERTa-v3 model with LoRA configuration using PEFT.\n",
    "    \"\"\"\n",
    "    # Load the base DeBERTa-v3 model\n",
    "    model_name = \"microsoft/deberta-v3-base\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,  # QQP is a binary classification task\n",
    "    )\n",
    "\n",
    "    # Freeze the base model's parameters\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Configure LoRA parameters\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,  # balance between parameter efficiency and the model's ability to learn.\n",
    "        lora_alpha=16,  # sufficient learning while avoiding overfitting\n",
    "        # target_modules=['query_proj', 'key_proj', 'value_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",  # no bias in the LoRA layers\n",
    "        task_type=\"SEQ_CLS\",  # Sequence Classification\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623c93c3-9403-4c1e-9cee-c7aff8d023e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = create_peft_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5f4bd3-11ca-4b81-b7ae-a773583a9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DebertaV2ForSequenceClassification(\n",
      "      (deberta): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (pooler): ContextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc357b86-5de1-4bbd-9af8-d907f0b6c853",
   "metadata": {},
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable layer: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14ed7f-6fbf-4bdf-a38a-79b837ac918a",
   "metadata": {},
   "source": [
    "Checking the number of training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4530a30b-330c-4e2a-b01d-fabdbe845c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 184,572,676 || trainable%: 0.0807\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daca260-9725-4470-b26d-14e270ba5b6f",
   "metadata": {},
   "source": [
    "As we can see from the message above, the following layers were initialized and therefore require updating during training:\n",
    "\n",
    "- `classifier.bias`\n",
    "- `classifier.weight`\n",
    "- `pooler.dense.bias`\n",
    "- `pooler.dense.weight`\n",
    "\n",
    "To ensure that these layers are trainable, we explicitly set their `requires_grad` attribute to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cce036-efd0-41fb-a7ab-aafea3fae019",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8c5bf-6a6d-4531-9a21-6121ecc3b86a",
   "metadata": {},
   "source": [
    "Again, checking the number of training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df773912-beef-476e-b661-bb10a321e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 741,124 || all params: 184,572,676 || trainable%: 0.4015\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd1d00-e68b-4d2f-8880-2def8d1721e2",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7baf999-0965-4be7-a63b-7b8649398c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b29e33-c00b-46bd-a3d6-80af39a29fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset():\n",
    "    \"\"\"\n",
    "    Load and preprocess the QQP dataset.\n",
    "    \"\"\"\n",
    "    # Load the QQP dataset from the GLUE benchmark\n",
    "    ds = load_dataset(\"glue\", \"qqp\")\n",
    "    # Filter out examples with missing labels\n",
    "    ds = ds.filter(lambda example: example[\"label\"] != -1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0801c0-e7df-47c1-b5e6-aba059017015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_and_prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b8dc94-2147-4cc6-9a29-6ed87eb3f31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f1c6c-f571-4205-87de-b3303304eeec",
   "metadata": {},
   "source": [
    "Example of different questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2793a0d-d73f-4ded-a3c3-a18d053d8d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the life of a math student? Could you describe your own experiences?\n",
      "\n",
      "\n",
      "Which level of prepration is enough for the exam jlpt5?\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"].select(range(1)).data[\"question1\"][0])\n",
    "print(\"\\n\")\n",
    "print(ds[\"train\"].select(range(1)).data[\"question2\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3234d5-b8f5-417b-8470-802a82212a46",
   "metadata": {},
   "source": [
    "Example of similar questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7e1bb3-4507-428c-a676-702561ac168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I control my horny emotions?\n",
      "\n",
      "\n",
      "How do you control your horniness?\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"].select(range(1)).data[\"question1\"][1])\n",
    "print(\"\\n\")\n",
    "print(ds[\"train\"].select(range(1)).data[\"question2\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539c3a9-e56b-4164-8c40-ee92e7743b49",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301c72b4-a861-4510-9eb7-19ee68bc01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Determine the best available device (CUDA, MPS, or CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA (GPU): {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Apple Silicon GPU)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bfaec69-03a3-4d39-b2ad-898420cbf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, ds, device):\n",
    "    \"\"\"\n",
    "    Tokenize the dataset using the provided tokenizer and move the data to the specified device.\n",
    "    This function skips empty splits and applies tokenization\n",
    "    and device movement to non-empty splits.\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the input question pairs\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"question1\"],\n",
    "            examples[\"question2\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "        # Include the labels in the output dictionary\n",
    "        tokenized[\"labels\"] = examples[\"label\"]\n",
    "        return tokenized\n",
    "\n",
    "    # Apply the preprocessing function to the dataset\n",
    "    # (this adds the tokenized columns like input_ids)\n",
    "    tokenized_ds = {}\n",
    "    for split in ds:\n",
    "        if len(ds[split]) > 0:\n",
    "            # Tokenize non-empty splits\n",
    "            tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "        else:\n",
    "            # Skip tokenization for empty splits (e.g., test set)\n",
    "            tokenized_ds[split] = ds[split]\n",
    "\n",
    "    # Now we can set the format to include the tokenized columns and labels\n",
    "    for split, dataset in tokenized_ds.items():\n",
    "        if len(dataset) > 0:  # Ensure the split is not empty before formatting\n",
    "            dataset.set_format(\n",
    "                type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "            )\n",
    "\n",
    "    # Move each dataset (train/validation) to the device\n",
    "    # (optional, but useful for GPU/CPU compatibility)\n",
    "    tokenized_ds[\"train\"] = tokenized_ds[\"train\"].with_format(\"torch\", device=device)\n",
    "    tokenized_ds[\"validation\"] = tokenized_ds[\"validation\"].with_format(\n",
    "        \"torch\", device=device\n",
    "    )\n",
    "\n",
    "    return tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48dfc8-ad4c-430a-9530-172d84611f89",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec643e60-b73f-44f4-a19e-de38c36aeb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/regokan/Desktop/Study/transfer-learning/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4cd69c7-e29b-4b42-ab72-b25b49d1e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb2a173-8aba-414f-a2fe-b87e71fffef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = preprocess_data(tokenizer, ds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c392bfe-6309-42a0-81fa-dd7860682934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['question1', 'question2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 363846\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['question1', 'question2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 40430\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['question1', 'question2', 'label', 'idx'],\n",
       "     num_rows: 0\n",
       " })}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8a2d3-30a1-4155-ad11-0daa5b0fd367",
   "metadata": {},
   "source": [
    "### **Cost Analysis for Training on `ml.p3.2xlarge` and `ml.g4dn.xlarge` Instances (Including Spot Instances)**\n",
    "\n",
    "This analysis estimates the cost of training a model with **741,124 trainable parameters** and **363,846 training rows** for **5 epochs** using AWS SageMaker’s **`ml.p3.2xlarge`** and **`ml.g4dn.xlarge`** instance types. Both **on-demand** and **spot instance** pricing are considered, providing a detailed breakdown of time and cost.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Key Information for the Estimate**\n",
    "\n",
    "- **Trainable Parameters**: 741,124\n",
    "- **Training Rows**: 363,846\n",
    "- **Number of Epochs**: 5\n",
    "- **Batch Size**: 32\n",
    "- **Steps per Epoch**: \n",
    "  \n",
    "  $$\\text{Steps per Epoch} = \\frac{363,846}{32} = 11,370 \\, \\text{steps/epoch}$$\n",
    "  \n",
    "- **Total Steps for 5 Epochs**:\n",
    "  \n",
    "  $$\\text{Total Steps} = 11,370 \\times 5 = 56,850 \\, \\text{steps}$$\n",
    "  \n",
    "\n",
    "### **2. Estimating Training Time per Step**\n",
    "\n",
    "- **`ml.p3.2xlarge` (NVIDIA V100 GPU)**:\n",
    "  - Estimated time per step: **0.20 seconds**.\n",
    "- **`ml.g4dn.xlarge` (NVIDIA T4 GPU)**:\n",
    "  - Estimated time per step: **0.35 seconds**.\n",
    "\n",
    "### **3. Total Training Time**\n",
    "\n",
    "#### **`ml.p3.2xlarge`**\n",
    "\n",
    "$$\\text{Total Time} = 56,850 \\times 0.20 \\, \\text{seconds} = 11,370 \\, \\text{seconds} = \\frac{11,370}{3600} \\approx 3.16 \\, \\text{hours}$$\n",
    "\n",
    "#### **`ml.g4dn.xlarge`**\n",
    "\n",
    "$$\\text{Total Time} = 56,850 \\times 0.35 \\, \\text{seconds} = 19,897.5 \\, \\text{seconds} = \\frac{19,897.5}{3600} \\approx 5.53 \\, \\text{hours}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. AWS Pricing (On-Demand and Spot Instances)**\n",
    "\n",
    "- **`ml.p3.2xlarge`**:\n",
    "  - **On-Demand**: \\$3.825 per hour.\n",
    "  - **Spot Instance**: Typically **70% lower**, approximately \\$1.15 per hour.\n",
    "\n",
    "- **`ml.g4dn.xlarge`**:\n",
    "  - **On-Demand**: \\$0.752 per hour.\n",
    "  - **Spot Instance**: Typically **70% lower**, approximately \\$0.226 per hour.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Cost Calculation**\n",
    "\n",
    "#### **On-Demand Pricing**\n",
    "\n",
    "##### **`ml.p3.2xlarge` (On-Demand)**:\n",
    "- **Training Time**: 3.16 hours.\n",
    "- **Cost per Hour**: $3.825.\n",
    "- **Total Cost**:\n",
    "\n",
    "$$\\text{Cost} = 3.16 \\times 3.825 = \\$12.08$$\n",
    "\n",
    "##### **`ml.g4dn.xlarge` (On-Demand)**:\n",
    "- **Training Time**: 5.53 hours.\n",
    "- **Cost per Hour**: $0.752.\n",
    "- **Total Cost**: $$\\text{Cost} = 5.53 \\times 0.752 = \\$4.16$$\n",
    "\n",
    "#### **Spot Instance Pricing**\n",
    "\n",
    "##### **`ml.p3.2xlarge` (Spot Instance)**:\n",
    "- **Training Time**: 3.16 hours.\n",
    "- **Cost per Hour**: $1.15 (70% discount).\n",
    "- **Total Cost**: $$\\text{Cost} = 3.16 \\times 1.15 = \\$3.63$$\n",
    "\n",
    "##### **`ml.g4dn.xlarge` (Spot Instance)**:\n",
    "- **Training Time**: 5.53 hours.\n",
    "- **Cost per Hour**: $0.226 (70% discount).\n",
    "- **Total Cost**: $$\\text{Cost} = 5.53 \\times 0.226 = \\$1.25$$\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Summary of Results**\n",
    "\n",
    "| **Instance Type**   | **Time per Epoch** | **Total Time (5 epochs)** | **Cost per Hour (On-Demand)** | **Total Cost (On-Demand)** | **Cost per Hour (Spot)** | **Total Cost (Spot)** |\n",
    "|---------------------|--------------------|---------------------------|-------------------------------|----------------------------|--------------------------|------------------------|\n",
    "| `ml.p3.2xlarge`     | 0.63 hours         | 3.16 hours                | \\$3.825                         | \\$12.08                     | \\$1.15                    | \\$3.63                  |\n",
    "| `ml.g4dn.xlarge`    | 1.11 hours         | 5.53 hours                | \\$0.752                         | \\$4.16                      | \\$0.226                   | \\$1.25                  |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Cost and Performance Comparison**\n",
    "\n",
    "#### **1. `ml.p3.2xlarge` (NVIDIA V100 GPU)**:\n",
    "- **On-Demand Cost**: \\$12.08\n",
    "- **Spot Instance Cost**: \\$3.63\n",
    "- **Training Time**: 3.16 hours\n",
    "- **Best For**: When **speed** is the priority, or when there is a need to train models frequently and quickly. However, the on-demand cost is significantly higher compared to spot instances.\n",
    "\n",
    "#### **2. `ml.g4dn.xlarge` (NVIDIA T4 GPU)**:\n",
    "- **On-Demand Cost**: \\$4.16\n",
    "- **Spot Instance Cost**: \\$1.25\n",
    "- **Training Time**: 5.53 hours\n",
    "- **Best For**: When **cost efficiency** is the main priority, and the additional training time (~2.4 hours longer than the `p3.2xlarge`) is acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Conclusion**\n",
    "\n",
    "- **Cost Efficiency**: If the additional training time (~2.4 hours) is acceptable, the **`ml.g4dn.xlarge`** on **spot instances** is the most cost-effective option, at only **\\$1.25** for the entire 5-epoch training job.\n",
    "- **Speed Efficiency**: For faster training, the **`ml.p3.2xlarge`** on **spot instances** completes the job in **3.16 hours** at a cost of **\\$3.63**, providing a good balance between speed and cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
